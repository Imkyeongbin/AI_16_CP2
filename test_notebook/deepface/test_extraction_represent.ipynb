{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nface detect, align 후 represent하는 코드\\nrepresent(표현) : embedding vectors의 list 반환. 이미지 자체가 아닌 vector를 이용해 동일인 여부를 구분할 것\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_extraction_mediapipe_representverify.ipynb\n",
    "\n",
    "'''\n",
    "face detect, align 후 represent하는 코드\n",
    "represent(표현) : embedding vectors의 list 반환. 이미지 자체가 아닌 vector를 이용해 동일인 여부를 구분할 것\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 259ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from deepface.basemodels import ArcFace, VGGFace, OpenFace, Facenet, Facenet512, FbDeepFace, DeepID, SFace\n",
    "\n",
    "# 모델 크기 딕셔너리\n",
    "TARGET_SIZES = {\n",
    "    \"VGGFace\": (224, 224),\n",
    "    \"Facenet\": (160, 160),\n",
    "    \"Facenet512\": (160, 160),\n",
    "    \"OpenFace\": (96, 96),\n",
    "    \"FbDeepFace\": (152, 152),\n",
    "    \"DeepID\": (55, 47),\n",
    "    \"Dlib\": (150, 150),\n",
    "    \"ArcFace\": (112, 112),\n",
    "    \"SFace\": (112, 112),\n",
    "}\n",
    "\n",
    "# 이미지 파일을 numpy 배열로 변환하는 함수\n",
    "def load_image(image_path, target_size):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize(target_size)  \n",
    "    img = np.array(img)  \n",
    "    return img\n",
    "\n",
    "# 이미지들의 임베딩 벡터를 계산하는 함수\n",
    "def get_embeddings(image_paths, model, target_size):\n",
    "    embeddings = list(map(lambda path: model.predict(np.expand_dims(load_image(path, target_size), axis=0)).flatten(), image_paths))\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# 이미지 파일 불러오기\n",
    "input_base_path = '/Users/jinmh/Desktop/face2'\n",
    "output_base_path = Path(input_base_path).parent / 'vectors2'\n",
    "\n",
    "image_paths = list(Path(input_base_path).rglob('*resized.png'))\n",
    "\n",
    "# 모델 로드 # 사용할 모델이름으로 작성 ######\n",
    "model_name = \"VGGFace\"\n",
    "target_size = TARGET_SIZES[model_name]\n",
    "model = eval(model_name + '.loadModel()')\n",
    "\n",
    "# 임베딩 벡터 계산\n",
    "embeddings = get_embeddings(image_paths, model, target_size)\n",
    "\n",
    "# 저장할 폴더 생성\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# 임베딩 벡터 저장\n",
    "for i, path in enumerate(image_paths):\n",
    "    filename = os.path.splitext(os.path.basename(path))[0] + '.npy'\n",
    "    output_path = output_base_path / filename\n",
    "    np.save(output_path, embeddings[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CP37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
